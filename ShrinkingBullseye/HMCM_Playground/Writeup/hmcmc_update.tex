\documentclass[12pt,a4paper]{article}
\linespread{1}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{wrapfig,lipsum,booktabs}
\usepackage{ragged2e}
\usepackage{amssymb}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{subcaption}
\usepackage[usenames, dvipsnames]{color}
\usepackage[draft]{graphicx}
\usepackage{float}
\usepackage{hhline}
\usepackage{tabularx}
\usepackage[citestyle=authoryear, bibstyle=authoryear,backend=biber]{biblatex}
\DeclareUnicodeCharacter{8208}{-}

\newtheorem{theorem}{Theorem}

\newcommand{\rojo}{\textcolor{red}}
\newcommand{\azul}{\textcolor{blue}}

\bibliography{ref.bib}
\title{Outlining a Posterior-Approximating HMCMC Algorithm}

\begin{document}
\maketitle
\section{Brief Update}
We would like to design an MCMC algorithm that combines Hamiltonian MCMC (HMCMC) \parencite{neal_mcmc_2012,betancourt_geometric_2014,betancourt_conceptual_2017} and the approximate posterior + refinement approach of the Shrinking Bullseye \parencite{conrad_accelerating_2015}. 

The basic skeleton of an HMCMC algorithm procedes as follows:
\begin{enumerate}
\item Consider the sampler at point $\vec{q}_n$ in parameter space with posterior density $\pi(\vec{q} | \vec{y})$ where $\vec{y}$ is the observed data (supressed going forward) .
\item Draw a momentum vector $\vec{p}_n$ from the distribution $\pi(\vec{p} | \vec{q} ) = \text{N}(\vec{p} |0, M)$ where $M$ is the mass matrix (in vanilla HMCMC this is usually chosen to be the covariance matrix of $\pi(\vec{q}|\vec{y})$ but for Riemanian HMCMC I believe one uses the Hessian of $\pi(\vec{q})$ with respect to $\vec{q}$ evaluated at $\vec{q}_n$, or something similar).
\item We then numerically integrate the following Hamiltonian with step size $h$ for $s$ steps (the integration time $sh$ is a free parameter for the sampler, and needs to be chosen wisely based on the geometry of the posterior), with initial conditions $(\vec{q}_n, \vec{p}_n)$:
\begin{equation}
\begin{split}
\dot{\vec{q}} &= M^{-1} \vec{p} \\ 
\dot{\vec{p}} &= - \nabla ln(\pi(\vec{q}))\\
\end{split}
\end{equation}
\item This integration is typically done by the leapfrog method, but any other \textit{symplectic} numerical integrator will work.  This takes the form:
\begin{equation}
\begin{split}
\vec{p}_{t+h/2} &= \vec{p}_t - (\frac{h}{2})  \nabla ln(\pi(\vec{q_t}))\\
\vec{q}_h &= \vec{q}_t + (h)M^{-1} \vec{p}_{t+h/2} \\ 
\vec{p}_h &= \vec{p}_{t+h/2} - (\frac{h}{2})  \nabla ln(\pi(\vec{q_{t+h}})) \\
\end{split}
\end{equation}
\item After integrating to time $sh$ the candidate points $q_{n+1}, p_{n+1}$ (supressing vector notation) are set as $q_{n+sh}, p_{n+sh}$.  To make the process reversible we need to now negate $p_{n+1}$, so our candidate points are $q_{n+1}, -p_{n+1}$.  These are then plugged into the usual Metropolis-Hastings accept/reject step.  This would not be necessary if we could integrate (1) without error, but due to the error in (2) the accept/reject step is necessary to prevent bias in the final Monte Carlo estimates.
\end{enumerate}

If we were to try and implement the above algorithm using the local regressions in Conrad et. al. \parencite{conrad_accelerating_2015} we would hit a problem when trying to compute $\nabla ln(\pi(q))$.  Letting $\hat{\pi}(q)$ denote the approximated/interpolated posterior, we note that when $\hat{\pi}$ is found using local approximations it is not necessarily continuous on the boundaries where the set of nearest neighbors changes, hence the derivative or gradient at any point along these boundaries is not defined/infinite.

If we're going to perform Hamiltonian MCMC on $\hat{\pi}$ we need an approximating algorithm that satisfies the following (loose) criteria:
\begin{enumerate}
\item Our interpolant must be at least once differentiable anywhere in the support of $\pi$ (twice if we'd like to do Riemanian HMCMC).  Furthermore it must be straightforward or cheap to evaluate $\nabla \hat{\pi}$ since we have to do so twice during a single evaulation of (2), and we are performing $s$ such evaluations per step of the sampler so that's $2s$ gradients evaluations per candidate proposal.
\item The algorithm has to be amenable to refinements.  Whatever interpolation scheme we use, it must allow us to test for refinements (ie. through something like the cross-validation approach in the Shrinking Bullseye), as well as add new points to the interpolating data set on an ad hoc basis (ie. whenever and \textit{wherever} the refinement criteria is triggered).
\item The Hamiltonian dynamics induced by $\hat{\pi}$ need to be similar enough to $\pi$ that the candidates proposed are actually good.  Following Betancourt's ``Conceptual Introduction to Hamiltonian Monte Carlo'', our interpolant needs to approximate the true posterior well on its \textit{typical set}.
\end{enumerate}

As far as I am aware, any interpolating scheme that requires gridded data would have a hard time incorporating refinements in a straightforward way; I am under the impression that one  would have to increase the grid resolution everywhere every time you add a refinement point which would quickly become expensive computationally.  Therefore, based on these criteria, I think we should be looking at Radial Basis Functions (RBFs) as our interpolating scheme.  These produce smooth interpolants on scattered data (ie. not on a grid), which meets criteria (1) and (2), and generally they work pretty well to approximate surfaces and their derivatives (working from the text \cite{buhmann_radial_2003} for error bounds).  The derivative can be computed analytically in terms of the interpolating coefficients and functional form of the RBFs, so one doesn't need to bother with a taking a numerical gradient or anything.  

I did find one other paper (haven't read it closely yet) where they use a neural network to approximate the posterior \parencite{zhang_hamiltonian_2015}.  Not sure if that would work better than RBFs or not, I'd like to implement their algorithm down the line and run some head-to-head comparisons.

Currently I think the best RBF for this task is the Thin Plate Spline (TPS).  The TPS allows us to penalize the wiggliness of the interpolant and I think this will produce the least amount of noise in the gradient of the log posterior, whereas another interpolation scheme might produce a lot of sharp peaks and valleys in the potential energy.  This could really spoil the behavior of the numerical Hamiltonian integrator, which doesn't work well when there is a high degree of curvature along the energy contours \parencite{betancourt_geometric_2014}.  However, fitting a TPS requires working with square matrices of the dimension of your interpolating dataset, which can rapidly become expensive, especially if we keep adding refinement points.  Therefore I am following the low-rank TPS approximation of Wood \parencite{wood_thin_2003} which claims to reduce the fitting cost.  Using TPSs the algorithm would be basically the same (except now using the interpolated potential energy/log posterior), however we would need to add a refinement-checking step and find a cheap way to update the interpolant with the new refinement.

I haven't investigated it as carefully yet, but I think we could also use a local regression approach to approximate both $\pi$ \textit{and} $\nabla \pi$.  It's not clear to me how well this would perform, but currently my plan is to implement both the TPS interpolation and the local regression approach and compare the two on some test problems.  I think that any such local scheme will run into continuity issues, so my priors don't expect this approach to work, but maybe it will.

I got in touch with Ian Grooms last week about this project, I think he has a good background (both stats and numerics) to give me advice on this project, plus I think he might see value in an algorithm that can sidestep expensive posteriors (I'm thinking some gigantic PDE weather model).  Unforunately he has been at a conference this week so I plan to meet with him next week and go over  go over my criteria and my understanding of RBFs.

\section{Goals for Next Week}
I've been reading through both Buhmann and Wood on RBFs and TPSs (respectively), but I still don't fully understand how they work.  I am in the process of implementing Wood's low-rank approximation to the TPS interpolant, and while this is almost finished it has been pretty slow going.  I hope to have that implementation up and running by the end of Monday, and once that's done it's pretty straightforward to run HMCMC using it, which will allow me to start playing around with refinement schemes and see what works best.  By the end of next week I hope to have a complete description of this algorithm (instead of just the outline presented above), and some comparisons to the alternative schemes I mentioned above.  Once this is done I can get started on proving ergodicity, convergence, etc. as well as checking for failure modes.  Based on the Shrinking Bullseye paper I think this scheme can be thought of as basically an adaptive MCMC, so I can hopefully use some of the Roberts and Rosenthal \cite{roberts_coupling_2007} results to get convergence.



\printbibliography

\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
